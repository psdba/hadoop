1.

install jdk
$ sudo apt install openjdk-8-jdk -y    

check the path
echo $JAVA_HOME

2.
sudo apt install openssh-server openssh-client -y

3.
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6-aarch64.tar.gz
5.tar xzf hadoop-3.3.6-aarch64.tar.gz
or with verbose
tar -zxvf hadoop-3.3.6-aarch64.tar.gz

4.Configure Hadoop Environment Variables (bashrc):

sudo nano .bashrc

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 
export PATH=$PATH:/usr/lib/jvm/java-8-openjdk-amd64/bin 
export HADOOP_HOME=~/hadoop-3.2.3/ 
export PATH=$PATH:$HADOOP_HOME/bin 
export PATH=$PATH:$HADOOP_HOME/sbin 
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME 
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native" 
export HADOOP_STREAMING=$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar
export HADOOP_LOG_DIR=$HADOOP_HOME/logs 
export PDSH_RCMD_TYPE=ssh

5.
sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
#Uncomment the $JAVA_HOME variable and add the full path to the OpenJDK installation on your system
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

6.edit file
sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml
 
#Copy the following to the file 
<configuration>
<property>
<name>hadoop.tmp.dir</name>
<value>/home/hdoop/tmpdata</value>
</property>
<property>
<name>fs.default.name</name>
<value>hdfs://127.0.0.1:9000</value>
</property>
</configuration>

7.
Create the temporary directory added in the previous step.

8.
Edit hdfs-site.xmlFile to define the NameNode and DataNode storage directories:

sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
 
#Add the following configuration to the file and, if needed, adjust the NameNode and DataNode directories to your custom locations.
<configuration>
<property>
<name>dfs.data.dir</name>
<value>/home/hdoop/dfsdata/namenode</value>
</property>
<property>
<name>dfs.data.dir</name>
<value>/home/hdoop/dfsdata/datanode</value>
</property>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
</configuration>

9.
Create the specific directories you defined for the dfs.data.dir value in above step
10.
Editmapred-site.xmlFile to define MapReduce values.
sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
 
#Add the following to the file
<configuration>
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>
</configuration>

11.Edit the yarn-site.xml file to define settings relevant to YARN. It contains configurations for the Node Manager, Resource Manager, Containers, and Application Master.

#Append the following configuration to the file<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
<name>yarn.resourcemanager.hostname</name>
<value>127.0.0.1</value>
</property>
<property>
<name>yarn.acl.enable</name>
<value>0</value>
</property>
<property>
<name>yarn.nodemanager.env-whitelist</name>
<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>
</configuration>

12.
Format HDFS NameNode:

#It is important to format the NameNode before starting Hadoop services for the first time
 
 hdfs namenode -format

13.
#Navigate to the directory
$ cd hadoop-3.3.6/sbin
 
#Execute
$ ./start-all.sh


set envrionment
1. cd hadoop-3.3.6
2.cd /etc/hadoop

accessing component
#Hadoop NameNode UI
http://localhost:9870
 
#DataNodes
http://localhost:9864
 
#YARN Resource Manager
http://localhost:8088

or ip address
hostname - I


3.ls

or

 sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
 
 and 
 
 JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 
 
 4.
 
 sudo nano core-site.xml
 <configuration> 
 <property> 
 <name>fs.defaultFS</name> 
 <value>hdfs://localhost:9000</value>  </property> 
 <property> 
<name>hadoop.proxyuser.dataflair.groups</name> <value>*</value> 
 </property> 
 <property> 
<name>hadoop.proxyuser.dataflair.hosts</name> <value>*</value> 
 </property> 
 <property> 
<name>hadoop.proxyuser.server.hosts</name> <value>*</value> 
 </property> 
 <property> 
<name>hadoop.proxyuser.server.groups</name> <value>*</value> 
 </property> 
</configuration>
 
 

====================
sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
cd hadoop-3.3.6/etc/hadoop

#Copy the following Hadoop Related Options
export HADOOP_HOME=/home/vboxuser/hadoop-3.3.6
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/nativ"

#Apply the changes to the current running environment
$ source ~/.bashrc
=====================
sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

add the following path
/usr/lib/jvm/java-8-openjdk-amd64


hadoop-3.3.6/bin/hdfs namenode -format

start the hadoop-3
cd hadoop-3.3.6/sbin
 ./start-all.sh
 
 
 troubleshoot
 ===================
 1.
 ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
 2.
 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
 3.
 chmod og-wx ~/.ssh/authorized_keys
 
 =======================
 export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
 
 ip address command
hostname -I 
 10.0.2.15 
 
 accessing node manager
 ========================
 http://10.0.2.15:8042/node
 
 
 resouce manager
 http://10.0.2.15:8088/





https://docs.celonis.com/en/hive--how-to-install-and-configure-apache-hadoop-and-hive-on-ubuntu.html